{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cifar10Experiment.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "QF12fURyNc_n",
        "WEtCICZAzF91"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "mount_file_id": "1xbSfhZjrNzW8BTLRWuhXi_6KIgpAvJYN",
      "authorship_tag": "ABX9TyOLBfTLUQaqlSOEuf4YMu3l",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FKhadivpour/Responsibility_CIFAR10/blob/main/Cifar10Experiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2M-O2uUVNDFq"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uoD7EUdjgXP"
      },
      "source": [
        "# Simple CNN model for CIFAR-10\n",
        "import numpy as np\n",
        "import os\n",
        "import keras\n",
        "from tensorflow.keras.datasets import cifar10, cifar100\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Input\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import callbacks\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras.layers import Convolution2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras import backend as K\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "%matplotlib inline\n",
        "import pickle\n",
        "import random\n",
        "import skimage\n",
        "from skimage.util import img_as_ubyte\n",
        "import skimage.transform\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.applications.efficientnet import EfficientNetB0\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "\n",
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoSRNWfrigOf",
        "outputId": "e719c72c-06ea-4c58-e7fb-3db08b5a0ac6"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.6.0'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQ3rPc6pNIKc"
      },
      "source": [
        "# Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jhjIwAhHodO"
      },
      "source": [
        "local_path = \"local/path/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52gDVW2BCYR4"
      },
      "source": [
        "# Load Data\n",
        "(X, y),(X_test,y_test) = cifar10.load_data()\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42)\n",
        "\n",
        "# Data Normalization\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_val = X_val.astype('float32')\n",
        "X_train = X_train/255.\n",
        "X_test = X_test/255.\n",
        "X_val = X_val/255.\n",
        "\n",
        "X_train_mean = np.mean(X_train)\n",
        "X_train -= X_train_mean\n",
        "X_test_mean = np.mean(X_test)\n",
        "X_test -= X_test_mean\n",
        "X_val_mean = np.mean(X_val)\n",
        "X_val -= X_val_mean\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "y_val = to_categorical(y_val)\n",
        "\n",
        "print(\"Train:\", X_train.shape, y_train.shape)\n",
        "print(\"Validation:\", X_val.shape, y_val.shape)\n",
        "print(\"Test:\", X_test.shape, y_test.shape)\n",
        "print(\"Input shape:\", X_train.shape[1:], \"Data type:\", X_train.dtype)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtmfXFJa6O6G"
      },
      "source": [
        "pickle.dump(X_train, open(local_path + \"X_train.pickle\", \"wb\"), protocol=4)\n",
        "pickle.dump(y_train, open(local_path + \"y_train.pickle\", \"wb\"), protocol=4)\n",
        "pickle.dump(X_val, open(local_path + \"X_val.pickle\", \"wb\"), protocol=4)\n",
        "pickle.dump(y_val, open(local_path + \"y_val.pickle\", \"wb\"), protocol=4)\n",
        "pickle.dump(X_test, open(local_path + \"X_test.pickle\", \"wb\"), protocol=4)\n",
        "pickle.dump(y_test, open(local_path + \"y_test.pickle\", \"wb\"), protocol=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1C8oYx3NL4j"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXFuLmd6bjpY"
      },
      "source": [
        "# Define Alexnet Model\n",
        "def build_model(input_shape,num_classes,model_type):\n",
        "  if model_type == \"costum\":\n",
        "      model = keras.models.Sequential()\n",
        "\n",
        "      model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=input_shape))\n",
        "      model.add(Dropout(0.2))\n",
        "\n",
        "      model.add(Conv2D(32,(3,3),padding='same', activation='relu'))\n",
        "      model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "      model.add(Conv2D(64,(3,3),padding='same',activation='relu'))\n",
        "      model.add(Dropout(0.2))\n",
        "\n",
        "      model.add(Conv2D(64,(3,3),padding='same',activation='relu'))\n",
        "      model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "      model.add(Conv2D(128,(3,3),padding='same',activation='relu'))\n",
        "      model.add(Dropout(0.2))\n",
        "\n",
        "      model.add(Conv2D(128,(3,3),padding='same',activation='relu'))\n",
        "      model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "      model.add(Flatten())\n",
        "      model.add(Dropout(0.2))\n",
        "      model.add(Dense(1024,activation='relu'))\n",
        "      model.add(Dropout(0.2))\n",
        "      model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "      return model\n",
        "\n",
        "\n",
        "  if model_type == 'alexnet':\n",
        "      model = keras.models.Sequential([\n",
        "          keras.layers.Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=input_shape),\n",
        "          keras.layers.BatchNormalization(),\n",
        "          keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
        "          keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n",
        "          keras.layers.BatchNormalization(),\n",
        "          keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
        "          keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
        "          keras.layers.BatchNormalization(),\n",
        "          keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
        "          keras.layers.BatchNormalization(),\n",
        "          keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
        "          keras.layers.BatchNormalization(),\n",
        "          keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
        "          keras.layers.Flatten(),\n",
        "          keras.layers.Dense(4096, activation='relu'),\n",
        "          keras.layers.Dropout(0.5),\n",
        "          keras.layers.Dense(4096, activation='relu'),\n",
        "          keras.layers.Dropout(0.5),\n",
        "          keras.layers.Dense(num_classes, activation='softmax')\n",
        "      ])\n",
        "\n",
        "      return model\n",
        "\n",
        "  elif model_type == \"costum_model2\":\n",
        "      model = keras.models.Sequential()\n",
        "\n",
        "      model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=input_shape))\n",
        "      model.add(Dropout(0.2))\n",
        "      model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "\n",
        "      model.add(Flatten())\n",
        "      model.add(Dropout(0.2))\n",
        "      model.add(Dense(64,activation='relu'))\n",
        "      model.add(Dropout(0.2))\n",
        "      model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "      return model\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "    if epoch == 0:\n",
        "      print(\"Epoch:\", epoch, \"Learning Rate:\", lr)\n",
        "      return lr \n",
        "    else:\n",
        "      print(\"Epoch:\", epoch, \"Learning Rate:\", lr * tf.math.exp(-0.2))\n",
        "      return  lr * tf.math.exp(-0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPjfFY5GNPuR"
      },
      "source": [
        "# Training Actor Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27rkS8X1gxBj"
      },
      "source": [
        "def train(args): \n",
        "\n",
        "  batch_size = args[\"batch_size\"]\n",
        "  verbose = args[\"verbose\"]\n",
        "  epochs = args[\"epochs\"]\n",
        "  input_shape = args[\"input_shape\"]\n",
        "  num_classes = args[\"num_classes\"]\n",
        "  learning_rate = args['learning_rate']\n",
        "  momentum = args['momentum']\n",
        "  model_type = args['model_type']\n",
        "  pre_train_lr = args['PreTrain_learningRate']\n",
        "  pre_train_epochs = args['PreTrain_epochs']\n",
        "\n",
        "  x_train = args[\"x_train\"] \n",
        "  y_train = args[\"y_train\"] \n",
        "  x_val = args[\"x_val\"] \n",
        "  y_val = args[\"y_val\"] \n",
        "\n",
        "  num_samples = (x_train.shape)[0]\n",
        "  total_delta_weights = [0] * num_samples\n",
        "\n",
        "  model = build_model(input_shape, num_classes, model_type)\n",
        "  \n",
        "  print(\"Training and calculating delta weights with batch size of\", batch_size)\n",
        "\n",
        "  opt = optimizers.SGD(learning_rate=learning_rate, momentum=momentum, nesterov=True)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  for epoch in range(epochs):\n",
        "    print(\"---------------------------------------------------\", \"Epoch:\", epoch+1, \"-----------------------------------------------------------------------\")\n",
        "    counter = 0\n",
        "    count = 0  \n",
        "    for i in range(num_samples):  \n",
        "        train_sample_x, train_sample_y = x_train[i], y_train[i]\n",
        "        train_sample_x = np.expand_dims(train_sample_x, axis=0)\n",
        "        train_sample_y = np.expand_dims(train_sample_y, axis=0)\n",
        "\n",
        "        pre_weights = np.array(model.layers[-1].get_weights()[0])\n",
        "\n",
        "        counter += 1\n",
        "\n",
        "        model.fit(train_sample_x, train_sample_y,\n",
        "                  batch_size=batch_size,\n",
        "                  shuffle=False, \n",
        "                  epochs=1,\n",
        "                  verbose=0)\n",
        "\n",
        "        post_weights = np.array(model.layers[-1].get_weights()[0])\n",
        "        delta_weights = np.subtract(post_weights, pre_weights)\n",
        "\n",
        "        total_delta_weights[i] = np.add(total_delta_weights[i], delta_weights)\n",
        "\n",
        "        if counter % 5000 == 0:\n",
        "          print(counter , \"samples seen!\")\n",
        "    \n",
        "    print(\"Train Scores:\")\n",
        "    tr_loss, tr_acc = model.evaluate(x=x_train, y=y_train, verbose=verbose)\n",
        "    print(\"Test Scores:\")\n",
        "    val_loss, val_acc = model.evaluate(x=x_val, y=y_val, verbose=verbose) \n",
        "\n",
        "  return model, total_delta_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulfL28cKJknz"
      },
      "source": [
        "my_args = {\"batch_size\": 1,\n",
        "\"epochs\": 5,\n",
        "\"verbose\": 1,\n",
        "\"num_classes\": y_train.shape[1],\n",
        "\"input_shape\": X_train.shape[1:],\n",
        "'learning_rate': 0.001,\n",
        "'momentum': 0.9,\n",
        "\"x_train\": X_train, \n",
        "\"y_train\": y_train,\n",
        "\"x_val\": X_val,\n",
        "\"y_val\": y_val,\n",
        "\"model_type\": 'costum',\n",
        "\"PreTrain_learningRate\": 0.0001,\n",
        "\"PreTrain_epochs\": 30\n",
        "}\n",
        "\n",
        "#  random.seed(73)\n",
        "actor, total_delta_weights = train(my_args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9-KY7rw4juF"
      },
      "source": [
        "actor.save_weights(local_path + \"model_1.h5')\n",
        "pickle.dump(total_delta_weights, open(local_path + \"total_delta_weights.pickle\", \"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7nc1FyTNTWF"
      },
      "source": [
        "# Responsibility"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIa_dzuzAZxB"
      },
      "source": [
        "model = build_model(num_classes=10, input_shape=[32,32,3], model_type='costum')\n",
        "model.load_weights(local_path + \"model_1.h5\")\n",
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7g_t_L3XnOx"
      },
      "source": [
        "total_delta_weights = pickle.load(open(local_path + \"total_delta_weights.pickle\", \"rb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKq06ko6TNaV"
      },
      "source": [
        "def responsibility(total_delta_weights):\n",
        "  arr = np.array(total_delta_weights, dtype=np.float16)\n",
        "  print(\"Layer delta weights:\", arr.shape)\n",
        "  print(\"----------------------------------------------------------------------\")\n",
        "\n",
        "  argmax_array = np.argmax(arr, axis=0)\n",
        "  print(\"Most Responsible Positive Array ---- Shape:\", argmax_array.shape)\n",
        "  counts_max = np.bincount(argmax_array.flatten())\n",
        "  print(\"Most Frequent Index:\", counts_max.argsort()[-10:][::-1])\n",
        "  print(\"----------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "  pickle.dump(argmax_array, open(local_path + \"array_of_IDs_of_positive_most_responsible_samples.pickle\", \"wb\"))\n",
        "\n",
        "  return argmax_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPCo745fRSK0"
      },
      "source": [
        "array_of_IDs_of_positive_most_responsible_samples = responsibility(total_delta_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcCQB-b1NYeH"
      },
      "source": [
        "# Generating Data for Model 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQ_7v8jHBP4F"
      },
      "source": [
        "## With Responsible Samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJLtSKhGTEtO"
      },
      "source": [
        "model = build_model(num_classes=10, input_shape=[32,32,3], model_type='costum')\n",
        "model.load_weights(local_path + \"AlexNetCifar10/model_1.h5\")\n",
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZYGXg4fMDxK"
      },
      "source": [
        "array_of_IDs_of_positive_most_responsible_samples = pickle.load(open(local_path + \"array_of_IDs_of_positive_most_responsible_samples.pickle\", \"rb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAjCL3_Izril"
      },
      "source": [
        "def shuffle_in_unison(a, b, c, d):\n",
        "    rng_state = np.random.get_state()\n",
        "    np.random.shuffle(a)\n",
        "    np.random.set_state(rng_state)\n",
        "    np.random.shuffle(b)\n",
        "    np.random.set_state(rng_state)\n",
        "    np.random.shuffle(c)\n",
        "    np.random.set_state(rng_state)\n",
        "    np.random.shuffle(d)\n",
        "\n",
        "    return a, b, c, d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QF12fURyNc_n"
      },
      "source": [
        "### Training Tuples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQbBxFYO0h2j"
      },
      "source": [
        "# Generating Data for Model 2\n",
        "predicted_y = model.predict(X_val, batch_size=1)\n",
        "residuals = np.argmax(predicted_y,1)!=np.argmax(y_val,1)\n",
        "loss = sum(residuals)/len(residuals)\n",
        "print(\"The Test 0/1 loss is: \", loss)\n",
        "\n",
        "print(\"Number of Incorrect samples:\", sum(residuals))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NNutshr0vJD"
      },
      "source": [
        "# Final Data for the Second Model \n",
        "\n",
        "incorrect_indecies = list([i for i, x in enumerate(residuals) if x])\n",
        "correct_indecies = [a for a in range(X_val.shape[0]) if a not in incorrect_indecies]\n",
        "\n",
        "print(len(incorrect_indecies), \"+\", len(correct_indecies), \"=\", len(X_val))\n",
        "\n",
        "\n",
        "incorrect_samples = []\n",
        "incorrect_x = []\n",
        "incorrect_y = []\n",
        "incorrect_y_true = []\n",
        "incorrect_y_pred = []\n",
        "for ind in incorrect_indecies:\n",
        "  incorrect_x.append(X_val[ind])\n",
        "  incorrect_y.append(0.0)\n",
        "  incorrect_y_true.append(y_val[ind])\n",
        "  incorrect_y_pred.append(predicted_y[ind])\n",
        "\n",
        "print(\"Incorrects Done! - Length =\", len(incorrect_x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TUvjmlbEFZG"
      },
      "source": [
        "correct_samples = []\n",
        "correct_x = []\n",
        "correct_y = []\n",
        "correct_y_true = []\n",
        "correct_y_pred = []\n",
        "for ind in correct_indecies:\n",
        "  correct_x.append(X_val[ind])\n",
        "  correct_y.append(1.0)\n",
        "  correct_y_true.append(y_val[ind])\n",
        "  correct_y_pred.append(predicted_y[ind])\n",
        "\n",
        "print(\"Corrects Done! - Length =\", len(correct_x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPeBQDZ-2Lkh"
      },
      "source": [
        "x_train_list = correct_x + incorrect_x\n",
        "y_train_list = correct_y + incorrect_y\n",
        "y_true_list = correct_y_true + incorrect_y_true\n",
        "y_pred_list = correct_y_pred + incorrect_y_pred\n",
        "\n",
        "final_x_train = np.array(x_train_list)\n",
        "final_y_train = np.array(y_train_list)\n",
        "final_y_true = np.array(y_true_list)\n",
        "final_y_pred = np.array(y_pred_list)\n",
        "\n",
        "final_x_train, final_y_train, final_y_true, final_y_pred = shuffle_in_unison(final_x_train, final_y_train, final_y_true, final_y_pred)\n",
        "\n",
        "print(\"Final X and Y Shape:\", final_x_train.shape, final_y_train.shape, final_y_true.shape, final_y_pred.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZH-kVgy22cl2"
      },
      "source": [
        "def finding_resp_samples_wx_method(sample, model, ind_array_fc_pos, training_samples):\n",
        "  inp = model.input  # input placeholder\n",
        "  outputs = [layer.output for layer in model.layers]  # all layer outputs\n",
        "  functor = K.function([inp], outputs)  # evaluation function\n",
        "  sample = np.reshape(sample, (1,32,32,3))\n",
        "  layer_outs = functor([sample])\n",
        "  dense_out = layer_outs[-1]\n",
        "  dense_in = layer_outs[-2].reshape((1024,))\n",
        "\n",
        "  max_ind_fc = np.argmax(dense_out)\n",
        "\n",
        "  fc_layer_weights = np.array(model.layers[-1].get_weights()[0])\n",
        "\n",
        "  max_weights = fc_layer_weights[:, max_ind_fc]\n",
        "\n",
        "\n",
        "  w = []\n",
        "  x = []\n",
        "  wx = []\n",
        "  for i in range(len(max_weights)):\n",
        "    w.append(max_weights[i])\n",
        "    x.append(dense_in[i])\n",
        "    wx.append(max_weights[i] * dense_in[i])\n",
        "\n",
        "\n",
        "  pos_most_resp_1 = training_samples[ind_array_fc_pos[np.argmax(wx), max_ind_fc]]\n",
        "\n",
        "  return pos_most_resp_1, wx\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPyPwPwF42J2"
      },
      "source": [
        "XY_tuples = []\n",
        "for i in range(len(final_x_train)):\n",
        "    sample = final_x_train[i]\n",
        "    prediction = final_y_train[i]\n",
        "    y_truee = final_y_true[i]\n",
        "    y_pred = final_y_pred[i]\n",
        "\n",
        "    pos_most_resp_1, wx = finding_resp_samples_wx_method(\n",
        "          sample, model,\n",
        "          array_of_IDs_of_positive_most_responsible_samples,\n",
        "          X_train)\n",
        "    \n",
        "    XY_tuples.append((sample, pos_most_resp_1, prediction, y_truee, y_pred, wx))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vq-bKi3v9NN4"
      },
      "source": [
        "pickle.dump(XY_tuples, open(local_path + \"train_tuples_responsibility.pickle\",\"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEtCICZAzF91"
      },
      "source": [
        "### Testing Tuples (Unseen)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSPS3ECJzJq8"
      },
      "source": [
        "# Generating Data for Model 2\n",
        "\n",
        "predicted_y_un = model.predict(X_test, batch_size=1)\n",
        "residuals_un = np.argmax(predicted_y_un,1)!=np.argmax(y_test,1)\n",
        "loss_un = sum(residuals_un)/len(residuals_un)\n",
        "print(\"The Test 0/1 loss is: \", loss_un)\n",
        "\n",
        "print(\"Number of Incorrect samples:\", sum(residuals_un))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjBH6BjxzJq-"
      },
      "source": [
        "# Generating Data for Model 2\n",
        "\n",
        "incorrect_indecies = list([i for i, x in enumerate(residuals_un) if x])\n",
        "correct_indecies = [a for a in range(X_test.shape[0]) if a not in incorrect_indecies]\n",
        "\n",
        "print(len(incorrect_indecies), \"+\", len(correct_indecies), \"=\", len(X_test))\n",
        "\n",
        "\n",
        "incorrect_samples = []\n",
        "incorrect_x = []\n",
        "incorrect_y = []\n",
        "incorrect_y_true = []\n",
        "incorrect_y_pred = []\n",
        "for ind in incorrect_indecies:\n",
        "  incorrect_x.append(X_test[ind])\n",
        "  incorrect_y.append(0.0)\n",
        "  incorrect_y_true.append(y_test[ind])\n",
        "  incorrect_y_pred.append(predicted_y_un[ind])\n",
        "\n",
        "\n",
        "\n",
        "correct_samples = []\n",
        "correct_x = []\n",
        "correct_y = []\n",
        "correct_y_true = []\n",
        "correct_y_pred = []\n",
        "for ind in correct_indecies:\n",
        "  correct_x.append(X_test[ind])\n",
        "  correct_y.append(1.0)\n",
        "  correct_y_true.append(y_test[ind])\n",
        "  correct_y_pred.append(predicted_y_un[ind])\n",
        "\n",
        "x_train_list = correct_x + incorrect_x\n",
        "y_train_list = correct_y + incorrect_y\n",
        "y_true_list = correct_y_true + incorrect_y_true\n",
        "y_pred_list = correct_y_pred + incorrect_y_pred\n",
        "\n",
        "final_x_train = np.array(x_train_list)\n",
        "final_y_train = np.array(y_train_list)\n",
        "final_y_true = np.array(y_true_list)\n",
        "final_y_pred = np.array(y_pred_list)\n",
        "\n",
        "final_x_train, final_y_train, final_y_true, final_y_pred = shuffle_in_unison(final_x_train, final_y_train, final_y_true, final_y_pred)\n",
        "\n",
        "print(\"Final X and Y Shape:\", final_x_train.shape, final_y_train.shape, final_y_true.shape, final_y_pred.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJ6oamPdMlup"
      },
      "source": [
        "def finding_resp_samples_wx_method(sample, model, ind_array_fc_pos, training_samples):\n",
        "  inp = model.input  # input placeholder\n",
        "  outputs = [layer.output for layer in model.layers]  # all layer outputs\n",
        "  functor = K.function([inp], outputs)  # evaluation function\n",
        "  sample = np.reshape(sample, (1,32,32,3))\n",
        "  layer_outs = functor([sample])\n",
        "  dense_out = layer_outs[-1]\n",
        "  dense_in = layer_outs[-2].reshape((1024,))\n",
        "\n",
        "  max_ind_fc = np.argmax(dense_out)\n",
        "\n",
        "  fc_layer_weights = np.array(model.layers[-1].get_weights()[0])\n",
        "\n",
        "  max_weights = fc_layer_weights[:, max_ind_fc]\n",
        "\n",
        "\n",
        "  w = []\n",
        "  x = []\n",
        "  wx = []\n",
        "  for i in range(len(max_weights)):\n",
        "    w.append(max_weights[i])\n",
        "    x.append(dense_in[i])\n",
        "    wx.append(max_weights[i] * dense_in[i])\n",
        "\n",
        "\n",
        "  pos_most_resp_1 = training_samples[ind_array_fc_pos[np.argmax(wx), max_ind_fc]]\n",
        "\n",
        "  return pos_most_resp_1, wx\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2rOHsQAzJrA"
      },
      "source": [
        "XY_tuples = []\n",
        "for i in range(len(final_x_train)):\n",
        "    sample = final_x_train[i]\n",
        "    prediction = final_y_train[i]\n",
        "    y_truee = final_y_true[i]\n",
        "    y_pred = final_y_pred[i]\n",
        "\n",
        "    pos_most_resp_1, wx = finding_resp_samples_wx_method(\n",
        "          sample, model,\n",
        "          array_of_IDs_of_positive_most_responsible_samples,\n",
        "          X_train)\n",
        "\n",
        "    XY_tuples.append((sample, pos_most_resp_1, prediction, y_truee, y_pred, wx))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FQHyIVUzJrC"
      },
      "source": [
        "pickle.dump(XY_tuples, open(local_path + \"tuples_unseen_responsibility.pickle\",\"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJs6govGQhn1"
      },
      "source": [
        "## With Nearest Neighbor Samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnEUrUheBhB5"
      },
      "source": [
        "train_tuples = pickle.load(open(local_path + \"train_tuples_responsibility.pickle\", \"rb\"))\n",
        "unseen_tuples = pickle.load(open(local_path + \"tuples_unseen_responsibility.pickle\", \"rb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldDwSuiT7TxW"
      },
      "source": [
        "def preprocess_data(train_tuples, unseen_tuples):\n",
        "\n",
        "  samples = []\n",
        "  resp_samples = []\n",
        "  y = []\n",
        "  y_trues = []\n",
        "  y_preds = []\n",
        "  for tpl in train_tuples:\n",
        "    samples.append(tpl[0])\n",
        "    resp_samples.append(tpl[1])\n",
        "    y_trues.append(tpl[3])\n",
        "    y_preds.append(tpl[4])\n",
        "\n",
        "    if tpl[2] == 0:\n",
        "      y.append([1.0,0.0])\n",
        "    elif tpl[2] == 1:\n",
        "      y.append([0.0,1.0])\n",
        "\n",
        "  unseen_samples = []\n",
        "  unseen_resp_samples = []\n",
        "  unseen_y = []\n",
        "  unseen_y_trues = []\n",
        "  unseen_y_preds = []\n",
        "  for unseen_tpl in unseen_tuples:\n",
        "    unseen_samples.append(unseen_tpl[0])\n",
        "    unseen_resp_samples.append(unseen_tpl[1])\n",
        "    unseen_y_trues.append(unseen_tpl[3])\n",
        "    unseen_y_preds.append(unseen_tpl[4])\n",
        "\n",
        "    if unseen_tpl[2] == 0:\n",
        "      unseen_y.append([1.0,0.0])\n",
        "    elif unseen_tpl[2] == 1:\n",
        "      unseen_y.append([0.0,1.0])\n",
        "\n",
        " \n",
        "  samples = np.array(samples)\n",
        "  resp_samples = np.array(resp_samples)\n",
        "\n",
        "  unseen_samples = np.array(unseen_samples)\n",
        "  unseen_resp_samples = np.array(unseen_resp_samples)\n",
        "\n",
        "  y = np.array(y)\n",
        "  unseen_y = np.array(unseen_y)\n",
        "\n",
        "\n",
        "\n",
        "  print('Training Data:')\n",
        "  print(\"Input Shapes:\", samples.shape, resp_samples.shape)\n",
        "  print(\"Output shape:\", y.shape)\n",
        "\n",
        "  print('Unseen Data:')\n",
        "  print(\"Input Image Shapes:\", unseen_samples.shape, unseen_resp_samples.shape)\n",
        "  print(\"Output shape:\", unseen_y.shape)\n",
        "  return samples, resp_samples, y, y_trues, y_preds, unseen_samples, unseen_resp_samples, unseen_y, unseen_y_trues, unseen_y_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSuguYOz7TxW"
      },
      "source": [
        "samples, resp_samples, y, y_trues, y_preds, samples_unseen, resp_samples_unseen, y_unseen, y_trues_unseen, y_preds_unseen = preprocess_data(train_tuples, unseen_tuples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZOtGAjgqqfg"
      },
      "source": [
        "def euc(flatten_img1, flatten_img2):\n",
        "\n",
        "  RH1 = Counter(flatten_img1)\n",
        "  RH2 = Counter(flatten_img2)\n",
        "\n",
        "  H1 = []\n",
        "  for i in range(128):\n",
        "      if i in RH1.keys():\n",
        "          H1.append(RH1[i])\n",
        "      else:\n",
        "          H1.append(0)\n",
        "  H2 = []\n",
        "  for i in range(128):\n",
        "      if i in RH2.keys():\n",
        "          H2.append(RH2[i])\n",
        "      else:\n",
        "          H2.append(0)\n",
        "\n",
        "  distance =0\n",
        "  for i in range(len(H1)):\n",
        "      distance += np.square(H1[i]-H2[i])\n",
        "  return np.sqrt(distance)\n",
        "\n",
        "def find_nearest_neighbor(sample, x_train, y_train, pred):\n",
        "  min_dist = np.inf\n",
        "  n = 0\n",
        "  for i in range(len(x_train)): \n",
        "    label = y_train[i]\n",
        "    if np.argmax(pred) == np.argmax(label):\n",
        "      img = x_train[i]  \n",
        "      if 3*(n+1)/3000 % 1 == 0:\n",
        "        print('{} out of {} Train Images.'.format(n + 1, 3000))\n",
        "      n += 1\n",
        "      # flatten_test_sample = get_flatten_img(sample)\n",
        "      # flatten_train_sample = get_flatten_img(img)\n",
        "      dist = euc(list(sample.flatten()), list(img.flatten()))\n",
        "      if dist < min_dist:\n",
        "        min_dist = dist\n",
        "        most_sim_img = img\n",
        "\n",
        "  return most_sim_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sz2JlMcY_f6N"
      },
      "source": [
        "### Training Tuples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVJN81UbtXyl"
      },
      "source": [
        "NN_samples = []\n",
        "for i in range(0,20000):\n",
        "  print(\"sample #\", i)\n",
        "  sample = samples[i]\n",
        "  pred = y_preds[i]\n",
        "  NN_sample = find_nearest_neighbor(sample, X_train, y_train, pred)\n",
        "  NN_samples.append(NN_sample)\n",
        "NN_samples = np.array(NN_samples)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zypZ-1_B7zVl"
      },
      "source": [
        "pickle.dump(NN_samples, open(local_path + \"NN_samples.pickle\",\"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8OCd7Rb8SED"
      },
      "source": [
        "train_tuples_NN = []\n",
        "for i in range(len(train_tuples)):\n",
        "  train_tuples_NN.append((train_tuples[i][0], NN_samples[i], train_tuples[i][2], train_tuples[i][3], train_tuples[i][4]))\n",
        "  if i%1000 == 0:\n",
        "    print(i, \"samples done!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEv94x748ulz"
      },
      "source": [
        "pickle.dump(train_tuples_NN, open(local_path + \"train_tuples_NN.pickle\",\"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeFMeZFZ_pWq"
      },
      "source": [
        "### Testing Tuples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rWGm8weJ_sLm"
      },
      "source": [
        "NN_samples_unseen = []\n",
        "for i in range(0,10000):\n",
        "  print(\"sample #\", i)\n",
        "  sample = samples_unseen[i]\n",
        "  pred = y_preds_unseen[i]\n",
        "  NN_sample = find_nearest_neighbor(sample, X_train, y_train, pred)\n",
        "  NN_samples_unseen.append(NN_sample)\n",
        "NN_samples_unseen = np.array(NN_samples_unseen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMof-eB7ArC7"
      },
      "source": [
        "pickle.dump(NN_samples_unseen, open(local_path + \"NN_samples_unseen.pickle\",\"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ARHTiVKLs5O"
      },
      "source": [
        "test_tuples_NN = []\n",
        "for i in range(len(tuples_unseen)):\n",
        "  test_tuples_NN.append((tuples_unseen[i][0], NN_samples_unseen[i], tuples_unseen[i][2], tuples_unseen[i][3], tuples_unseen[i][4]))\n",
        "  if i%1000 == 0:\n",
        "    print(i, \"samples done!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZr86FdfLv74"
      },
      "source": [
        "pickle.dump(test_tuples_NN, open(local_path + \"tuples_unseen_NN.pickle\",\"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRgwIQlRN4Qa"
      },
      "source": [
        "# Model 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPkvvmr7SWBo"
      },
      "source": [
        "## With Responsible Samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMMEc-rMx_zt"
      },
      "source": [
        "train_tuples = pickle.load(open(local_path + \"train_tuples_responsibility.pickle\", \"rb\"))\n",
        "unseen_tuples = pickle.load(open(local_path + \"tuples_unseen_responsibility.pickle\", \"rb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCRbWYHYHcK4"
      },
      "source": [
        "# What's inside the tuples: [sample, pos_most_resp_1, prediction, y_truee, y_pred, wx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0K04_ErINWTQ"
      },
      "source": [
        "def preprocess_data(train_tuples, unseen_tuples):\n",
        "\n",
        "  samples = []\n",
        "  resp_samples = []\n",
        "  y = []\n",
        "  y_trues = []\n",
        "  y_preds = []\n",
        "  for tpl in train_tuples:\n",
        "    samples.append(tpl[0])\n",
        "    resp_samples.append(tpl[1])\n",
        "    y_trues.append(tpl[3])\n",
        "    y_preds.append(tpl[4])\n",
        "\n",
        "    if tpl[2] == 0:\n",
        "      y.append([1.0,0.0])\n",
        "    elif tpl[2] == 1:\n",
        "      y.append([0.0,1.0])\n",
        "\n",
        "  unseen_samples = []\n",
        "  unseen_resp_samples = []\n",
        "  unseen_y = []\n",
        "  unseen_y_trues = []\n",
        "  unseen_y_preds = []\n",
        "  for unseen_tpl in unseen_tuples:\n",
        "    unseen_samples.append(unseen_tpl[0])\n",
        "    unseen_resp_samples.append(unseen_tpl[1])\n",
        "    unseen_y_trues.append(unseen_tpl[3])\n",
        "    unseen_y_preds.append(unseen_tpl[4])\n",
        "\n",
        "    if unseen_tpl[2] == 0:\n",
        "      unseen_y.append([1.0,0.0])\n",
        "    elif unseen_tpl[2] == 1:\n",
        "      unseen_y.append([0.0,1.0])\n",
        "\n",
        " \n",
        "  samples = np.array(samples)\n",
        "  resp_samples = np.array(resp_samples)\n",
        "\n",
        "  unseen_samples = np.array(unseen_samples)\n",
        "  unseen_resp_samples = np.array(unseen_resp_samples)\n",
        "\n",
        "  y = np.array(y)\n",
        "  unseen_y = np.array(unseen_y)\n",
        "\n",
        "\n",
        "\n",
        "  print('Training Data:')\n",
        "  print(\"Input Shapes:\", samples.shape, resp_samples.shape)\n",
        "  print(\"Output shape:\", y.shape)\n",
        "\n",
        "  print('Unseen Data:')\n",
        "  print(\"Input Image Shapes:\", unseen_samples.shape, unseen_resp_samples.shape)\n",
        "  print(\"Output shape:\", unseen_y.shape)\n",
        "  return samples, resp_samples, y, y_trues, y_preds, unseen_samples, unseen_resp_samples, unseen_y, unseen_y_trues, unseen_y_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BE5a_P4lOPpz"
      },
      "source": [
        "samples, resp_samples, y, y_trues, y_preds, samples_unseen, resp_samples_unseen, y_unseen, y_trues_unseen, y_preds_unseen = preprocess_data(train_tuples, unseen_tuples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbpABm6rV9dS"
      },
      "source": [
        "def train_model_2(args): \n",
        "  batch_size = args[\"batch_size\"]\n",
        "  maxepoch = args[\"maxepoches\"]\n",
        "  num_classes = args[\"num_classes\"]\n",
        "  model_type = args[\"model_type\"]\n",
        "  input_shape = args[\"input_shape\"]\n",
        "\n",
        "  min_lr = args[\"min_lr\"]\n",
        "  max_lr = args[\"max_lr\"]\n",
        "  optimizer = args[\"optimizer\"]\n",
        "\n",
        "  train_samples = args[\"train_samples\"]\n",
        "  test_samples = args[\"test_samples\"]\n",
        "  train_resps = args[\"train_resps\"]\n",
        "  test_resps = args[\"test_resps\"]\n",
        "  y_train = args[\"y_train\"]\n",
        "  y_test = args[\"y_test\"]\n",
        "\n",
        "  model = None\n",
        "\n",
        "  def lr_scheduler(epoch):\n",
        "      if epoch <= 0.6 * maxepoch:\n",
        "        learning_rate = max_lr\n",
        "      else:\n",
        "        learning_rate = max_lr + ((epoch + 1 - (0.6 * maxepoch)) * (min_lr - max_lr) / (0.4 * maxepoch) )\n",
        "      print(\"Epoch:\", epoch, \"Learning Rate:\", learning_rate)\n",
        "      return learning_rate\n",
        "\n",
        "  model = build_model(input_shape, num_classes, model_type)\n",
        "  reduce_lr = callbacks.LearningRateScheduler(lr_scheduler)\n",
        "  if optimizer == \"sgd\":\n",
        "    opt = optimizers.SGD(learning_rate=min_lr, momentum=0.9, nesterov=True)\n",
        "  elif optimizer == \"adam\":\n",
        "    opt = optimizers.Adam(learning_rate=min_lr)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "  model.summary()\n",
        "\n",
        "  x_train = np.concatenate((train_samples, train_resps), axis=3)\n",
        "  history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    # steps_per_epoch=samples.shape[0] // batch_size,\n",
        "                    shuffle=True, \n",
        "                    epochs=maxepoch,\n",
        "                    validation_split=0.2,\n",
        "                    callbacks=[reduce_lr],\n",
        "                    verbose=1)\n",
        "\n",
        "  return model, history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SieLxLWOlhd"
      },
      "source": [
        "my_args = {\"batch_size\": 32,\n",
        "\"maxepoches\": 20,\n",
        "\"num_classes\": y.shape[1],\n",
        "\"model_type\": \"costum_model2\",\n",
        "\"input_shape\": [32,32,6],\n",
        "\n",
        "\"min_lr\": 0.00001,\n",
        "\"max_lr\": 0.0001,\n",
        "\"optimizer\": \"adam\", # different optimizers: adam, sgd\n",
        "\n",
        "\n",
        "\"train_samples\": samples, \n",
        "\"test_samples\": samples_unseen,\n",
        "\"train_resps\": resp_samples,\n",
        "\"test_resps\": resp_samples_unseen,\n",
        "\"y_train\": y,\n",
        "\"y_test\": y_unseen\n",
        "}\n",
        "\n",
        "critic, hist = train_model_2(my_args)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vuKSCql5s_A"
      },
      "source": [
        "# list all data in history\n",
        "print(hist.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(hist.history['accuracy'])\n",
        "plt.plot(hist.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(hist.history['loss'])\n",
        "plt.plot(hist.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwg0YSxj52wJ"
      },
      "source": [
        "print(\"Evaluation on Unseen Dataset:\")\n",
        "\n",
        "x_test = np.concatenate((samples_unseen, resp_samples_unseen), axis=3)\n",
        "print(x_test.shape)\n",
        "\n",
        "critic.evaluate(x_test, y_unseen)\n",
        "y_hat = critic.predict(x_test)\n",
        "\n",
        "y_pred = np.argmax(y_hat, axis=1)\n",
        "y_true = np.argmax(y_unseen, axis=1)\n",
        "\n",
        "my_precision = precision_score(y_true, y_pred)\n",
        "my_recal = recall_score(y_true, y_pred)\n",
        "my_f1 = f1_score(y_true, y_pred)\n",
        "print(\"Precision:\", my_precision, \"Recall:\", my_recal, \"f1 score:\", my_f1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6z-hydFR6War"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(y_true, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbiB_dkHSnMl"
      },
      "source": [
        "## With Nearest Neighbor Samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDmjt_6nSrxR"
      },
      "source": [
        "train_tuples = pickle.load(open(local_path + \"train_tuples_NN.pickle\", \"rb\"))\n",
        "unseen_tuples_random = pickle.load(open(local_path + \"tuples_unseen_NN.pickle\", \"rb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ly8TclG2SrxT"
      },
      "source": [
        "# What's inside the tuples: [sample, pos_most_resp_1, prediction, y_truee, y_pred, wx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRGMsrnzSrxT"
      },
      "source": [
        "def preprocess_data(train_tuples, unseen_tuples):\n",
        "\n",
        "  samples = []\n",
        "  resp_samples = []\n",
        "  y = []\n",
        "  y_trues = []\n",
        "  y_preds = []\n",
        "  for tpl in train_tuples:\n",
        "    samples.append(tpl[0])\n",
        "    resp_samples.append(tpl[1])\n",
        "    y_trues.append(tpl[3])\n",
        "    y_preds.append(tpl[4])\n",
        "\n",
        "    if tpl[2] == 0:\n",
        "      y.append([1.0,0.0])\n",
        "    elif tpl[2] == 1:\n",
        "      y.append([0.0,1.0])\n",
        "\n",
        "  unseen_samples = []\n",
        "  unseen_resp_samples = []\n",
        "  unseen_y = []\n",
        "  unseen_y_trues = []\n",
        "  unseen_y_preds = []\n",
        "  for unseen_tpl in unseen_tuples:\n",
        "    unseen_samples.append(unseen_tpl[0])\n",
        "    unseen_resp_samples.append(unseen_tpl[1])\n",
        "    unseen_y_trues.append(unseen_tpl[3])\n",
        "    unseen_y_preds.append(unseen_tpl[4])\n",
        "\n",
        "    if unseen_tpl[2] == 0:\n",
        "      unseen_y.append([1.0,0.0])\n",
        "    elif unseen_tpl[2] == 1:\n",
        "      unseen_y.append([0.0,1.0])\n",
        "\n",
        " \n",
        "  samples = np.array(samples)\n",
        "  resp_samples = np.array(resp_samples)\n",
        "\n",
        "  unseen_samples = np.array(unseen_samples)\n",
        "  unseen_resp_samples = np.array(unseen_resp_samples)\n",
        "\n",
        "  y = np.array(y)\n",
        "  unseen_y = np.array(unseen_y)\n",
        "\n",
        "\n",
        "\n",
        "  print('Training Data:')\n",
        "  print(\"Input Shapes:\", samples.shape, resp_samples.shape)\n",
        "  print(\"Output shape:\", y.shape)\n",
        "\n",
        "  print('Unseen Data:')\n",
        "  print(\"Input Image Shapes:\", unseen_samples.shape, unseen_resp_samples.shape)\n",
        "  print(\"Output shape:\", unseen_y.shape)\n",
        "  return samples, resp_samples, y, y_trues, y_preds, unseen_samples, unseen_resp_samples, unseen_y, unseen_y_trues, unseen_y_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g096Y7PuSrxT"
      },
      "source": [
        "samples, random_samples, y, y_trues, y_preds, samples_unseen, random_samples_unseen, y_unseen, y_trues_unseen, y_preds_unseen = preprocess_data(train_tuples_random, unseen_tuples_random)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIfZzw_qSrxT"
      },
      "source": [
        "def train_model_2(args): \n",
        "  batch_size = args[\"batch_size\"]\n",
        "  maxepoch = args[\"maxepoches\"]\n",
        "  num_classes = args[\"num_classes\"]\n",
        "  model_type = args[\"model_type\"]\n",
        "  input_shape = args[\"input_shape\"]\n",
        "\n",
        "  min_lr = args[\"min_lr\"]\n",
        "  max_lr = args[\"max_lr\"]\n",
        "  optimizer = args[\"optimizer\"]\n",
        "\n",
        "  train_samples = args[\"train_samples\"]\n",
        "  test_samples = args[\"test_samples\"]\n",
        "  train_resps = args[\"train_resps\"]\n",
        "  test_resps = args[\"test_resps\"]\n",
        "  y_train = args[\"y_train\"]\n",
        "  y_test = args[\"y_test\"]\n",
        "\n",
        "  model = None\n",
        "\n",
        "  def lr_scheduler(epoch):\n",
        "      if epoch <= 0.6 * maxepoch:\n",
        "        learning_rate = max_lr\n",
        "      else:\n",
        "        learning_rate = max_lr + ((epoch + 1 - (0.6 * maxepoch)) * (min_lr - max_lr) / (0.4 * maxepoch) )\n",
        "      print(\"Epoch:\", epoch, \"Learning Rate:\", learning_rate)\n",
        "      return learning_rate\n",
        "\n",
        "  model = build_model(input_shape, num_classes, model_type)\n",
        "  reduce_lr = callbacks.LearningRateScheduler(lr_scheduler)\n",
        "  if optimizer == \"sgd\":\n",
        "    opt = optimizers.SGD(learning_rate=min_lr, momentum=0.9, nesterov=True)\n",
        "  elif optimizer == \"adam\":\n",
        "    opt = optimizers.Adam(learning_rate=min_lr)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "  model.summary()\n",
        "\n",
        "  x_train = np.concatenate((train_samples, train_resps), axis=3)\n",
        "  history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    shuffle=True, \n",
        "                    epochs=maxepoch,\n",
        "                    validation_split=0.2,\n",
        "                    callbacks=[reduce_lr],\n",
        "                    verbose=1)\n",
        "\n",
        "\n",
        "  return model, history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhI-lHSXSrxU"
      },
      "source": [
        "my_args = {\"batch_size\": 32,\n",
        "\"maxepoches\": 20,\n",
        "\"num_classes\": y.shape[1],\n",
        "\"model_type\": \"costum_model2\",\n",
        "\"input_shape\": [32,32,6],\n",
        "\n",
        "\"min_lr\": 0.00001,\n",
        "\"max_lr\": 0.0001,\n",
        "\"optimizer\": \"adam\", # different optimizers: adam, sgd\n",
        "\n",
        "\n",
        "\"train_samples\": samples, \n",
        "\"test_samples\": samples_unseen,\n",
        "\"train_resps\": random_samples,\n",
        "\"test_resps\": random_samples_unseen,\n",
        "\"y_train\": y,\n",
        "\"y_test\": y_unseen\n",
        "}\n",
        "\n",
        "critic_NN, hist_NN = train_model_2(my_args)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpXWTUaZSrxU"
      },
      "source": [
        "# list all data in history\n",
        "print(hist_NN.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(hist_NN.history['accuracy'])\n",
        "plt.plot(hist_NN.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(hist_NN.history['loss'])\n",
        "plt.plot(hist_NN.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJICiMmzSrxU"
      },
      "source": [
        "print(\"Evaluation on Unseen Dataset:\")\n",
        "\n",
        "x_test = np.concatenate((samples_unseen, random_samples_unseen), axis=3)\n",
        "print(x_test.shape)\n",
        "\n",
        "critic_NN.evaluate(x_test, y_unseen)\n",
        "y_hat = critic_NN.predict(x_test)\n",
        "\n",
        "y_pred = np.argmax(y_hat, axis=1)\n",
        "y_true = np.argmax(y_unseen, axis=1)\n",
        "\n",
        "my_precision = precision_score(y_true, y_pred)\n",
        "my_recal = recall_score(y_true, y_pred)\n",
        "my_f1 = f1_score(y_true, y_pred)\n",
        "print(\"Precision:\", my_precision, \"Recall:\", my_recal, \"f1 score:\", my_f1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzajXi6TSrxU"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(y_true, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeDelftN6VPR"
      },
      "source": [
        "## With Just Original Samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBBT7oPk6bKc"
      },
      "source": [
        "train_tuples = pickle.load(open(local_path + \"train_tuples.pickle\", \"rb\"))\n",
        "unseen_tuples_random = pickle.load(open(local_path + \"tuples_unseen.pickle\", \"rb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BhTTCzN6bKd"
      },
      "source": [
        "# What's inside the tuples: [sample, pos_most_resp_1, prediction, y_truee, y_pred, wx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlGSFS1G6bKd"
      },
      "source": [
        "def preprocess_data(train_tuples, unseen_tuples):\n",
        "\n",
        "  samples = []\n",
        "  resp_samples = []\n",
        "  y = []\n",
        "  y_trues = []\n",
        "  y_preds = []\n",
        "  for tpl in train_tuples:\n",
        "    samples.append(tpl[0])\n",
        "    resp_samples.append(tpl[1])\n",
        "    y_trues.append(tpl[3])\n",
        "    y_preds.append(tpl[4])\n",
        "\n",
        "    if tpl[2] == 0:\n",
        "      y.append([1.0,0.0])\n",
        "    elif tpl[2] == 1:\n",
        "      y.append([0.0,1.0])\n",
        "\n",
        "  unseen_samples = []\n",
        "  unseen_resp_samples = []\n",
        "  unseen_y = []\n",
        "  unseen_y_trues = []\n",
        "  unseen_y_preds = []\n",
        "  for unseen_tpl in unseen_tuples:\n",
        "    unseen_samples.append(unseen_tpl[0])\n",
        "    unseen_resp_samples.append(unseen_tpl[1])\n",
        "    unseen_y_trues.append(unseen_tpl[3])\n",
        "    unseen_y_preds.append(unseen_tpl[4])\n",
        "\n",
        "    if unseen_tpl[2] == 0:\n",
        "      unseen_y.append([1.0,0.0])\n",
        "    elif unseen_tpl[2] == 1:\n",
        "      unseen_y.append([0.0,1.0])\n",
        "\n",
        " \n",
        "  samples = np.array(samples)\n",
        "  resp_samples = np.array(resp_samples)\n",
        "\n",
        "  unseen_samples = np.array(unseen_samples)\n",
        "  unseen_resp_samples = np.array(unseen_resp_samples)\n",
        "\n",
        "  y = np.array(y)\n",
        "  unseen_y = np.array(unseen_y)\n",
        "\n",
        "\n",
        "\n",
        "  print('Training Data:')\n",
        "  print(\"Input Shapes:\", samples.shape, resp_samples.shape)\n",
        "  print(\"Output shape:\", y.shape)\n",
        "\n",
        "  print('Unseen Data:')\n",
        "  print(\"Input Image Shapes:\", unseen_samples.shape, unseen_resp_samples.shape)\n",
        "  print(\"Output shape:\", unseen_y.shape)\n",
        "  return samples, resp_samples, y, y_trues, y_preds, unseen_samples, unseen_resp_samples, unseen_y, unseen_y_trues, unseen_y_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UR8uw_tr6bKd"
      },
      "source": [
        "samples, samples, y, y_trues, y_preds, samples_unseen, samples_unseen, y_unseen, y_trues_unseen, y_preds_unseen = preprocess_data(train_tuples_random, unseen_tuples_random)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RA3bI73a6bKe"
      },
      "source": [
        "def train_model_2(args): \n",
        "  batch_size = args[\"batch_size\"]\n",
        "  maxepoch = args[\"maxepoches\"]\n",
        "  num_classes = args[\"num_classes\"]\n",
        "  model_type = args[\"model_type\"]\n",
        "  input_shape = args[\"input_shape\"]\n",
        "\n",
        "  min_lr = args[\"min_lr\"]\n",
        "  max_lr = args[\"max_lr\"]\n",
        "  optimizer = args[\"optimizer\"]\n",
        "\n",
        "  train_samples = args[\"train_samples\"]\n",
        "  test_samples = args[\"test_samples\"]\n",
        "  train_resps = args[\"train_resps\"]\n",
        "  test_resps = args[\"test_resps\"]\n",
        "  y_train = args[\"y_train\"]\n",
        "  y_test = args[\"y_test\"]\n",
        "\n",
        "  model = None\n",
        "\n",
        "  def lr_scheduler(epoch):\n",
        "      if epoch <= 0.6 * maxepoch:\n",
        "        learning_rate = max_lr\n",
        "      else:\n",
        "        learning_rate = max_lr + ((epoch + 1 - (0.6 * maxepoch)) * (min_lr - max_lr) / (0.4 * maxepoch) )\n",
        "      print(\"Epoch:\", epoch, \"Learning Rate:\", learning_rate)\n",
        "      return learning_rate\n",
        "\n",
        "  model = build_model(input_shape, num_classes, model_type)\n",
        "  reduce_lr = callbacks.LearningRateScheduler(lr_scheduler)\n",
        "  if optimizer == \"sgd\":\n",
        "    opt = optimizers.SGD(learning_rate=min_lr, momentum=0.9, nesterov=True)\n",
        "  elif optimizer == \"adam\":\n",
        "    opt = optimizers.Adam(learning_rate=min_lr)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "  model.summary()\n",
        "\n",
        "  x_train = train_samples \n",
        "  history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    # steps_per_epoch=samples.shape[0] // batch_size,\n",
        "                    shuffle=True, \n",
        "                    epochs=maxepoch,\n",
        "                    validation_split=0.2,\n",
        "                    callbacks=[reduce_lr],\n",
        "                    verbose=1)\n",
        "\n",
        "  return model, history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwGrUPnL6bKe"
      },
      "source": [
        "my_args = {\"batch_size\": 32,\n",
        "\"maxepoches\": 20,\n",
        "\"num_classes\": y.shape[1],\n",
        "\"model_type\": \"costum_model2\",\n",
        "\"input_shape\": [32,32,3],\n",
        "\n",
        "\"min_lr\": 0.00001,\n",
        "\"max_lr\": 0.0001,\n",
        "\"optimizer\": \"adam\", # different optimizers: adam, sgd\n",
        "\n",
        "\n",
        "\"train_samples\": samples, \n",
        "\"test_samples\": samples_unseen,\n",
        "\"train_resps\": samples,\n",
        "\"test_resps\": samples_unseen,\n",
        "\"y_train\": y,\n",
        "\"y_test\": y_unseen\n",
        "}\n",
        "\n",
        "model_2_original, hist_original = train_model_2(my_args)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTM6P9Va6bKe"
      },
      "source": [
        "# list all data in history\n",
        "print(hist_original.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(hist_original.history['accuracy'])\n",
        "plt.plot(hist_original.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(hist_original.history['loss'])\n",
        "plt.plot(hist_original.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrRKJyE86bKe"
      },
      "source": [
        "print(\"Evaluation on Unseen Dataset:\")\n",
        "\n",
        "# x_test = np.concatenate((samples_unseen, random_samples_unseen), axis=3)\n",
        "x_test = samples_unseen\n",
        "print(x_test.shape)\n",
        "\n",
        "model_2_original.evaluate(x_test, y_unseen)\n",
        "y_hat = model_2_original.predict(x_test)\n",
        "\n",
        "y_pred = np.argmax(y_hat, axis=1)\n",
        "y_true = np.argmax(y_unseen, axis=1)\n",
        "\n",
        "my_precision = precision_score(y_true, y_pred)\n",
        "my_recal = recall_score(y_true, y_pred)\n",
        "my_f1 = f1_score(y_true, y_pred)\n",
        "print(\"Precision:\", my_precision, \"Recall:\", my_recal, \"f1 score:\", my_f1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDx5ivED6bKe"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(y_true, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}